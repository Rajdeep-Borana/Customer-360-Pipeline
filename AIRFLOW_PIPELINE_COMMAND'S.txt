from airflow import DAG
from airflow.utils.dates import days_ago
from airflow.utils.dates import timedelta
from airflow.sensors.http_sensor import HttpSensor
from airflow.operators.bash_operator import BashOperator
from airflow.contrib.hooks.ssh_hook import SSHHook
from airflow.contrib.operators.ssh_operator import SSHOperator
#from airflow.hooks import BashOperator
#from airflow.operators import BashOperator
from airflow.operators.dummy_operator import DummyOperator
#
from airflow import settings
from airflow.models import Connection
from airflow.contrib.hooks.sqoop_hook
from airflow.contrib.operators.datastore_import_operator
from airflow.contrib.operators.vertica_to_mysql
from airflow.models
from airflow.models.connection
##
from airflow.contrib.operators.slack_webhook_operator import SlackWebhookOperator
from airflow.contrib.operators.slack_webhook_operator import SlackWebhookOperator


dag = DAG(
	dag_id = 'Customer_360_Pipline_College',
	start_date = days_ago(1)
	)

sensor = HttpSensor(
	task_id = 'watch_for_orders',
	http_conn_id = 'order_s3',
	endpoint = 'orders.csv',
	response_check = lambda response: response.status_code == 200,
	dag = dag,
	retry_delay = timedelta(minutes=5),
	retries = 12
)

def get_order_url():
	session =  settings.Session()
	connection = session.query(Connection).filter(Connection.conn_id == 'order_s3').first()
	return f'{connection.schema}://{connection.host}/orders.csv'

download_order_cmd = f'rm -rf airflow_pipeline && mkdir -p airflow_pipeline && cd airflow_pipeline && wget {get_order_url()}'

download_to_edgenode = SSHOperator(
	task_id = 'download_orders',
	ssh_conn_id = 'cloudera',
	command = download_order_cmd,
	dag = dag
)

def fetch_customer_info_cmd():
	command_one = "hive -e 'DROP TABLE airflow.customers'"
	command_one_ext = 'hdfs dfs -rm -R  /user/hive/warehouse/airflow.db/customers'
	command_two = "sqoop import --connect jdbc:mysql://quickstart.cloudera:3306/retail_db --username root --password cloudera --table customers --warehouse-dir /user/hive/warehouse --hive-import --create-hive-table --hive-table airflow.customers --fields-terminated-by \',\'"                
	command_three = "exit 0"
	return f'{command_one} && {command_one_ext} && ({command_two} || {command_three})'

import_cust_to_hive_txns = SSHOperator(
 task_id='sqoop_import_customers_txns_hive',
 ssh_conn_id='cloudera',
 command=fetch_customer_info_cmd(),
 dag=dag
)

upload_orders_info = SSHOperator(
	task_id = 'upload_orders_to_hdfs',
	ssh_conn_id = 'cloudera',
	command = 'hdfs dfs -rm -R -f airflow_input && hdfs dfs -mkdir -p airflow_input && hdfs dfs -put ./airflow_pipeline/orders.csv airflow_input/',
	dag = dag
)


def get_order_filter_cmd():
	command_zero = 'export SPARK_MAJOR_VERSION=2'
	command_one = 'hdfs dfs -rm -R -f airflow_output'
	command_two = 'spark-submit --class Filter_Orders Airflow_bundell.jar airflow_input/orders.csv airflow_output'
	return f'{command_zero} && {command_one} && {command_two}'

process_orders_info = SSHOperator(
	task_id = 'process_orders',
	ssh_conn_id = 'cloudera',
	command = get_order_filter_cmd(),
	dag = dag
)

def create_order_hive_table_cmd():
	return 'hive -e "CREATE external table if not exists airflow.orders(order_id int, order_date string, customer_id int, status string) '
	'row format delimited fields terminated by \',\' stored as textfile location \'/user/cloudera/airflow_output/\'"'

create_order_table = SSHOperator(
	task_id = 'create_orders_table_hive',
	ssh_conn_id = 'cloudera',
	command = create_order_hive_table_cmd(),
	dag = dag
)

def load_hbase_cmd():
	command_one = 'hive -e "create table if not exists airflow_hbase(customer_id int, customer_fname string, customer_lname string, order_id int, orders_date string)
			CLUSTERED BY (customer_id) into 8 BUCKETS
			STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
			with SERDEPROPERTIES
			('hbase.columns.mapping'=':key,personal:customer_fname,
			personal:customer_lname,
			personal:order_id,
			personal:order_date')
			TBLPROPERTIES("hbase.table.name"="Airflow_HBase");"'
	command_two = 'insert overwrite table airflow.airflow_hbase select c.customer_id, c.customer_fname, c.customer_lname, o.order_id, '
	'o.order_date from airflow.customers c join default.orders o ON (c.customer_id = o.customer_id)'
	return f'{command_one} && {command_two}'

load_hbase = SSHOperator(
	task_id = 'load_hbase_table',
	ssh_conn_id = 'cloudera',
	command = load_hbase_cmd(),
	dag = dag
)

dummy = DummyOperator(
 task_id='dummy',
 dag = dag 
)


def slack_password():
	return 'Rajdeep@1234'

success_notify = SlackWebhookOperator(
	task_id = 'success_notify',
	http_conn_id = 'slack_webhook',
	message = 'Data loaded successfully in HBase',
	username = 'rbsingh9111',
	webhook_token = slack_password(),
	dag = dag
)

failure_notify = SlackWebhookOperator(
	task_id = 'failure_notify',
	http_conn_id = 'slack_webhook',
	message = 'Data loaded failure for HBase',
	#channel = '#customer-360-notification',
	username = 'rbsingh9111',
	webhook_token = slack_password(),
	dag = dag,
	trigger_rule = 'all_failed'
)


sensor >> import_cust_to_hive_txns

sensor >> download_to_edgenode >> upload_orders_info >> process_orders_info >> create_order_table

[import_cust_to_hive_txns , create_order_table ] >> load_hbase >> dummy

dummy >> [success_notify , failure_notify]


